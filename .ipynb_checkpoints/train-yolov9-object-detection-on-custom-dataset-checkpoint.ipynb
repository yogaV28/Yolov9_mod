<<<<<<< HEAD
version https://git-lfs.github.com/spec/v1
oid sha256:91af2d15ae70e624d322b7dc90b5dd64155659f02c87ade9245295370359655a
size 109237
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQjdUKvQigN2"
   },
   "source": [
    "# How to Train YOLOv9 on a Custom Dataset\n",
    "---\n",
    "\n",
    "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov9-model/)\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/WongKinYiu/yolov9)\n",
    "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/XHT2c8jT3Bc)\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2402.13616-b31b1b.svg)](https://arxiv.org/pdf/2402.13616.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m09A8n4djDwY"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hX88yficL7",
    "outputId": "43bfb50e-0aa8-4ce4-cf74-389545fe8357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 19:40:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     Off | 00000000:2B:00.0  On |                  N/A |\n",
      "| 30%   42C    P5              19W / 160W |   1382MiB /  8188MiB |     35%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTprsNjHja4l"
   },
   "source": [
    "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rowKDIT-jJ9k",
    "outputId": "e15d746f-d9dc-4cad-d3cd-946d49783189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/srmist/Desktop/yolov9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWRGGT7Zjjbq"
   },
   "source": [
    "## Clone and Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WyY-fboBLZB"
   },
   "source": [
    "**NOTE:** YOLOv9 is very new. At the moment, we recommend using a fork of the main repository. The `detect.py` script contains a bug that prevents inference. This bug is patched in the fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pixgo4qnjdoU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/SkalskiP/yolov9.git\n",
    "%cd yolov9\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcx7KoNzqpgz"
   },
   "source": [
    "**NOTE:** Let's install the [`roboflow`](https://pypi.org/project/roboflow) package, which we will use to download our dataset from [Roboflow Universe](https://universe.roboflow.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPGqlohQqgAO"
   },
   "outputs": [],
   "source": [
    "!pip install -q roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8oLIkX2l2P0"
   },
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FieRuZnB4wH"
   },
   "source": [
    "**NOTE:** In the YOLOv9 paper, versions `yolov9-s` and `yolov9-m` are also mentioned, but the weights for these models are not yet available in the YOLOv9 [repository](https://github.com/WongKinYiu/yolov9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h7j3aUE7l1Je"
   },
   "outputs": [],
   "source": [
    "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au6np1JS8eRB",
    "outputId": "ddf7bd59-9fd0-43a2-e743-9a52852930ce"
   },
   "outputs": [],
   "source": [
    "!ls -la {HOME}/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  1 09:56:57 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 32%   32C    P8               9W / 170W |    376MiB / 12288MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1168      G   /usr/lib/xorg/Xorg                           45MiB |\r\n",
      "|    0   N/A  N/A      1731      G   /usr/lib/xorg/Xorg                          106MiB |\r\n",
      "|    0   N/A  N/A      1858      G   /usr/bin/gnome-shell                         73MiB |\r\n",
      "|    0   N/A  N/A      3128      G   /usr/lib/firefox/firefox                    131MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dg29vEyLkTDA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIKNGnN2kcTp"
   },
   "source": [
    "**NOTE:** If you want to run inference using your own file as input, simply upload image to Google Colab and update `SOURCE_IMAGE_PATH` with the path leading to your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUiPMLxmj4Ze"
   },
   "outputs": [],
   "source": [
    "!wget -P {HOME}/data -q https://media.roboflow.com/notebooks/examples/dog.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiqFDio2kX8i"
   },
   "outputs": [],
   "source": [
    "SOURCE_IMAGE_PATH = f\"{HOME}/dog.jpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dlfABN6m-LL"
   },
   "source": [
    "## Detection with pre-trained COCO model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EPCiYcFComZ"
   },
   "source": [
    "### gelan-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzGyLetWoTWp",
    "outputId": "c51e7975-c75f-474a-e217-6e070f4176f8"
   },
   "outputs": [],
   "source": [
    "!python3 detect.py --weights weights/gelan-c.pt --conf 0.1 --source car-detection-2-1/test/images --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hflXfkBt3N0k"
   },
   "source": [
    "**NOTE:** By default, the results of each subsequent inference sessions are saved in `{HOME}/yolov9/runs/detect/`, in directories named `exp`, `exp2`, `exp3`, ... You can override this behavior by using the `--name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sAE1P1BxpHYL",
    "outputId": "14444f6d-0f1d-48f0-c31f-d70cea929072"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f\"yolov9/runs/detect/exp\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCEIP-jFCsRN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## yolov9-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEQALIFaCuoX",
    "outputId": "65539362-623f-4217-d714-fdfe6bd28827"
   },
   "outputs": [],
   "source": [
    "!python detect.py --weights {HOME}/weights/yolov9-c.pt --conf 0.1 --source {HOME}/data/dog.jpeg --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "llm4xIE_CyXJ",
    "outputId": "3b222b26-5668-401b-c1de-4f524bd841a9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f\"{HOME}/yolov9/runs/detect/exp2/dog.jpeg\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7fZKrxsq_td",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Authenticate and Download the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5yx2GkI2P7Q"
   },
   "source": [
    "**NOTE:** The dataset must be saved inside the `{HOME}/yolov9` directory, otherwise, the training will not succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyLpftfU2Q1U"
   },
   "outputs": [],
   "source": [
    "%cd {HOME}/yolov9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eosmGt89vMO1"
   },
   "source": [
    "**NOTE:** In this tutorial, I will use the [football-players-detection](https://universe.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc) dataset. Feel free to replace it with your dataset in YOLO format or use another dataset available on [Roboflow Universe](https://universe.roboflow.com). Additionally, if you plan to deploy your model to Roboflow after training, make sure you are the owner of the dataset and that no model is associated with the version of the dataset you are going to training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J3s_2_7p_gn"
   },
   "outputs": [],
   "source": [
    "'''import roboflow\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "rf = roboflow.Roboflow()\n",
    "\n",
    "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
    "version = project.version(8)\n",
    "dataset = version.download(\"yolov9\")\n",
    "'''\n",
    "dataset = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTbGpF2IsZ24"
   },
   "source": [
    "## Train Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N68Bdf4FsMYW"
   },
   "outputs": [],
   "source": [
    "dataset = 'car detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N68Bdf4FsMYW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '{HOME}/yolov9'\n",
      "/home/srmist/Desktop/yolov9\n",
      "2024-06-01 09:55:49.842461: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-01 09:55:50.073341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 09:55:50.668068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraagavdharun111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\n",
      "                [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--imgsz IMGSZ]\n",
      "                [--rect] [--resume [RESUME]] [--nosave] [--noval]\n",
      "                [--noautoanchor] [--noplots] [--evolve [EVOLVE]]\n",
      "                [--bucket BUCKET] [--cache [CACHE]] [--image-weights]\n",
      "                [--device DEVICE] [--multi-scale] [--single-cls]\n",
      "                [--optimizer {SGD,Adam,AdamW,LION}] [--sync-bn]\n",
      "                [--workers WORKERS] [--project PROJECT] [--name NAME]\n",
      "                [--exist-ok] [--quad] [--cos-lr] [--flat-cos-lr] [--fixed-lr]\n",
      "                [--label-smoothing LABEL_SMOOTHING] [--patience PATIENCE]\n",
      "                [--freeze FREEZE [FREEZE ...]] [--save-period SAVE_PERIOD]\n",
      "                [--seed SEED] [--local_rank LOCAL_RANK]\n",
      "                [--min-items MIN_ITEMS] [--close-mosaic CLOSE_MOSAIC]\n",
      "                [--entity ENTITY] [--upload_dataset [UPLOAD_DATASET]]\n",
      "                [--bbox_interval BBOX_INTERVAL]\n",
      "                [--artifact_alias ARTIFACT_ALIAS]\n",
      "train.py: error: unrecognized arguments: detection/car-detection-2-1}/data.yaml\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/yolov9\n",
    "\n",
    "!python train.py \\\n",
    "--batch 16 --epochs 10 --img 640 --device 0 --min-items 0 --close-mosaic 15 \\\n",
    "--data {car detection/car-detection-2-1}/data.yaml \\\n",
    "--weights {weights/gelan-c.pt}/weights/gelan-c.pt \\\n",
    "--cfg models/detect/gelan-c.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "N68Bdf4FsMYW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/srmist/Desktop/yolov9\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /home/srmist/Downloads/ls/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (2.5.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (2.29.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (2.3.1)\n",
      "Requirement already satisfied: setproctitle in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from wandb) (67.8.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "2024-06-01 14:58:18.411791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-01 14:58:18.437467: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 14:58:18.838541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraagavdharun111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=weights/gelan-c.pt, cfg=models/detect/gelan-c.yaml, data=idd-detection/IDD_Detection/data.yaml, hyp=hyp.scratch-high.yaml, epochs=100, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=True, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=True, single_cls=False, optimizer=AdamW, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 v0.1-89-g93f1a28 Python-3.11.3 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12044MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/srmist/Desktop/yolov9/wandb/run-20240601_145820-zqho9da1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-durian-43\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/raagavdharun111/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/raagavdharun111/YOLOv5/runs/zqho9da1\u001b[0m\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 22      [15, 18, 21]  1   5521480  models.yolo.DDetect                     [40, [256, 512, 512]]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gelan-c summary: 621 layers, 25467912 parameters, 25467896 gradients\n",
      "\n",
      "Transferred 931/937 items from weights/gelan-c.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/la\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images/0007776.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images/0017291.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4723      1.3486]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/val/labels\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/val/images/0008384.jpg: 1 duplicate labels removed\n",
      "Plotting labels to runs/train/exp44/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp44\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/1146 00:00/home/srmist/Desktop/yolov9/train.py:295: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
      "       0/99      3.84G      1.465      5.084      1.238        201        704:  /home/srmist/Desktop/yolov9/train.py:295: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
      "       0/99      9.03G      2.139      2.296      1.626        189        512: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.38      0.115        0.1      0.049\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/99      10.6G      2.044       2.02      1.568        150        960: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.501      0.132      0.136     0.0718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/99      10.6G      1.898      1.767      1.471        230        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.537      0.155      0.166     0.0921\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/99      10.6G      1.842      1.691      1.444        314        704: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.617      0.169      0.179      0.102\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/99      10.6G      1.776       1.58      1.404        120        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.61       0.18      0.193      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/99      10.6G      1.735      1.531      1.386        210        512: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.62      0.185      0.209      0.122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/99      10.6G      1.689      1.459      1.349         82        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.694      0.194      0.212      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/99      10.6G      1.661      1.416      1.337        165        960: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.606      0.194      0.217      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/99      10.6G      1.637      1.374      1.322        220        896: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.671      0.199      0.229      0.135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/99      10.6G      1.618      1.342      1.313        410        768: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.548      0.208      0.237      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/99      10.6G        1.6      1.312      1.302        135        736: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.756      0.212      0.244      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/99      10.6G      1.585      1.299      1.292        188        704: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.634      0.218      0.247      0.146\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/99      10.6G      1.561      1.262      1.282        219        896: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.615       0.23      0.252       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/99      8.47G      1.547      1.241      1.266        134        576: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.621      0.219      0.249      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/99      8.47G      1.541       1.24      1.271        197        608: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.699      0.234      0.261      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/99      8.48G      1.527       1.22      1.264        179        576: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.621      0.234      0.255      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/99      8.48G      1.525        1.2      1.253        115        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.673      0.236      0.258      0.155\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/99      8.48G      1.507       1.18      1.252        169        512: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.657      0.241      0.271      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/99      8.48G      1.488      1.162      1.235        112        960: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.687      0.237      0.272      0.166\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/99      8.48G      1.484      1.161      1.244        151        320: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.748      0.244      0.278      0.169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/99      8.48G      1.494      1.143      1.235        136        448: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.613      0.252      0.276      0.166\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/99      8.48G      1.468      1.134      1.231        198        352: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.691      0.248      0.283      0.171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/99      8.48G      1.462      1.115      1.218        136        512: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.679       0.25      0.283      0.172\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/99      8.48G      1.447      1.092      1.211        181        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.672      0.252      0.331      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/99      8.48G      1.444      1.102      1.208        202        576: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.622      0.257      0.287      0.176\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/99      8.48G      1.432      1.083      1.209        167        512: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.69      0.257      0.325      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/99      8.48G      1.421      1.083      1.204        357        864: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.678      0.255      0.301      0.186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/99      8.48G      1.421      1.067      1.196        109        704: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.717      0.261      0.304      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/99      8.48G      1.415      1.055      1.197        118        768: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.676      0.268      0.309      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/99      8.48G      1.409      1.047       1.19        294        768: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.682      0.267        0.3      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/99      8.48G      1.414       1.06      1.198        114        928: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.702      0.261      0.301      0.185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/99      8.48G      1.397      1.037      1.192        129        864: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.661      0.262      0.301      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/99      8.48G      1.394      1.032      1.186        228        544: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.719      0.272      0.303      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/99      8.48G      1.386      1.023      1.186        233        448: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774        0.7      0.285      0.311      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/99      8.48G      1.376      1.013      1.175        215        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.69      0.275      0.308      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/99      8.48G      1.376      1.006      1.175        109        832: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.737      0.277      0.331      0.206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/99      8.48G      1.375      1.023      1.181        196        384: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.73      0.281      0.317      0.194\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/99      8.48G      1.368     0.9933      1.166        147        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.753      0.273      0.315      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/99      8.48G      1.365      0.996      1.169        227        352: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.697      0.281      0.318      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/99      10.1G       1.35     0.9848      1.165        191        864: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.693      0.285       0.32      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/99      10.1G       1.35     0.9716      1.156        173        544: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.594      0.357      0.361      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/99      10.1G      1.352      0.977      1.161        211        896: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.731      0.284      0.343      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/99      10.1G      1.346     0.9671      1.157        202        544: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.754      0.283      0.339      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/99      10.1G       1.34     0.9545      1.153        256        320: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.769       0.29      0.331      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      44/99      10.1G      1.339     0.9586      1.153         86        800: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.728      0.294      0.348      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/99      10.1G      1.335     0.9558      1.152        116        608: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.758      0.296      0.357      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/99      10.1G      1.329     0.9454      1.147        130        896: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.761      0.291      0.353      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/99      10.1G      1.328     0.9444      1.149        218        544: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774       0.72      0.296      0.335      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/99      10.1G      1.323     0.9386      1.142        172        352: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.745      0.295      0.346       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/99      10.1G      1.319     0.9386      1.145        173        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.664      0.297      0.336      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/99      10.1G      1.311     0.9246      1.138        129        960: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.735      0.296      0.347      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      51/99      10.1G      1.311     0.9268      1.145        175        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.737      0.298      0.374      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      52/99      10.1G      1.294     0.9181      1.133        288        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.745      0.297      0.378      0.237\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      53/99      10.1G       1.31     0.9299      1.145        164        960: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.741        0.3      0.381      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      54/99      10.1G      1.299     0.9108      1.132        251        864: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1310      30774      0.616      0.343      0.374      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      55/99      10.1G      1.293     0.9197      1.144        135        864:  "
     ]
    }
   ],
   "source": [
    "%cd /home/srmist/Desktop/yolov9\n",
    "%pip install wandb\n",
    "\n",
    "\n",
    "\n",
    "!python3 train.py \\\n",
    "--batch 4 --epochs 100 --img 640 --device 0 --min-items 0 --multi-scale   \\\n",
    "--data idd-detection/IDD_Detection/data.yaml --optimizer AdamW --noautoanchor  \\\n",
    "--weights weights/gelan-c.pt \\\n",
    "--cfg models/detect/gelan-c.yaml \\\n",
    "--hyp hyp.scratch-high.yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confTreshold: 0.45              # Detection confidence threshold updated to where F1 score peaks\n",
    "nmsTreshold : 0.45              # nms threshold remains the same\n",
    "maxSupportBatchSize: 1          # Support max input batch size remains the same\n",
    "quantizationInfer: \"INT8\"       # Support FP32 or FP16 quantization remains the same\n",
    "onnxFile: \"yolov9-c.onnx\"       # The currently used onnx model file remains the same\n",
    "engineFile: \"yolov9-c.engine\"   # Automatically generate file names for the Tensorrt inference engine remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorrt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtrt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcuda\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import pycuda\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load(\"/home/srmist/Desktop/yolov9/best.onnx\")\n",
    "\n",
    "# Create TensorRT engine from ONNX model\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "    builder.max_workspace_size = 1 << 28  # 256MiB\n",
    "    builder.max_batch_size = 1\n",
    "    builder.fp16_mode = False  # Set to False for INT8 quantization\n",
    "    builder.int8_mode = True\n",
    "    builder.int8_calibrator = ...  # Set INT8 calibrator if necessary\n",
    "    parser.parse(onnx_model.SerializeToString())\n",
    "\n",
    "    # Set detection and NMS thresholds\n",
    "    network.get_output(0).set_dynamic_range(-1.0, 1.0)\n",
    "    network.get_output(1).set_dynamic_range(-1.0, 1.0)\n",
    "    network.get_output(2).set_dynamic_range(-1.0, 1.0)\n",
    "    network.get_output(3).set_dynamic_range(0.0, 255.0)\n",
    "    network.get_output(4).set_dynamic_range(0.0, 255.0)\n",
    "    network.get_output(5).set_dynamic_range(0.0, 255.0)\n",
    "\n",
    "    # Build TensorRT engine\n",
    "    engine = builder.build_cuda_engine(network)\n",
    "\n",
    "    # Save TensorRT engine to file\n",
    "    with open(\"/home/srmist/Desktop/yolov9/best.engine\", \"wb\") as f:\n",
    "        f.write(engine.serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (671477444.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    mkdir configs\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mkdir configs\n",
    "mv ~/home/srmist/Desktop/yolov9/best.onxx configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Collecting torch==2.3.0 (from torchvision)\n",
      "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.3.0->torchvision)\n",
      "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sympy in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0->torchvision)\n",
      "  Downloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/srmist/Downloads/ls/lib/python3.11/site-packages (from sympy->torch==2.3.0->torchvision) (1.2.1)\n",
      "Installing collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchvision-0.18.0 triton-2.3.0 typing-extensions-4.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "clearml.browser_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N68Bdf4FsMYW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade clearml tensorboard ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpCwjSUg2Mrw",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Examine Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHsMq7wc3bve"
   },
   "source": [
    "**NOTE:** By default, the results of each subsequent training sessions are saved in `{HOME}/yolov9/runs/train/`, in directories named `exp`, `exp2`, `exp3`, ... You can override this behavior by using the `--name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WslwgMAW2Euc",
    "outputId": "144b5202-56a6-4782-f797-763010939665"
   },
   "outputs": [],
   "source": [
    "!ls {HOME}/yolov9/runs/train/exp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "grirpuCstpZE",
    "outputId": "cd9b75ea-4451-4493-8a48-6ca4e848f075"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%cd /home/srmist/Desktop/yolov9/\n",
    "\n",
    "Image(filename=f\"runs/train/exp9/results.png\", width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "qggEg7Hv1zJ6",
    "outputId": "9fcd5ba4-da6c-45d7-a051-b0b88d7da06d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f\"runs/train/exp9/confusion_matrix.png\", width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "Xja2fjTl32Ml",
    "outputId": "dc6b1c52-b42b-4e7b-c64d-11647e6aead9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f\"runs/exp9/val_batch0_pred.jpg\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih1rk9O_4CYG"
   },
   "source": [
    "## Validate Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoZv8kNE4NxS",
    "outputId": "a13455e7-0524-46e5-c99e-4d930065b140"
   },
   "outputs": [],
   "source": [
    "%cd {HOME}/yolov9\n",
    "\n",
    "!python val.py \\\n",
    "--img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 \\\n",
    "--data {dataset.location}/data.yaml \\\n",
    "--weights {HOME}/yolov9/runs/train/exp/weights/best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJJ5fiqT6mEq"
   },
   "source": [
    "## Inference with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vnrn9cwIsUs"
   },
   "outputs": [],
   "source": [
    "!python detect.py \\\n",
    "--img 1280 --conf 0.1 --device 0 \\\n",
    "--weights {HOME}/yolov9/runs/train/exp/weights/best.pt \\\n",
    "--source {dataset.location}/test/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPbhTtVXtM4Q"
   },
   "source": [
    "**NOTE:** Just like behore, the inference results have been saved in the appropriate directory inside `{HOME}/yolov9/runs/detect/`. Let's examine few of those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "XoV4sGOKJPZj",
    "outputId": "35982356-6054-4497-ca3e-b35fe7377c01"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for image_path in glob.glob(f'{HOME}/yolov9/runs/detect/exp3/*.jpg')[:2]:\n",
    "      display(Image(filename=image_path, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMTTVZU48DdJ"
   },
   "source": [
    "## BONUS: Deploy YOLOv9 Model with Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoDQAk5arRfm"
   },
   "source": [
    "**NOTE:** To deploy the model and display inference results, we will need two additional packages - [`inference`](https://pypi.org/project/inference) and [`supervision`](https://pypi.org/project/supervision). Let's install and import them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn6YWeaa8bdZ"
   },
   "outputs": [],
   "source": [
    "!pip install -q inference supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BauaNyA8wrj"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import getpass\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "from inference import get_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu0-mgYpskPY"
   },
   "source": [
    "**NOTE:** Before using your model in Inference, you first need to upload your weights to Roboflow Universe. Ensure to specify the correct `model_type` - `yolov9`, and that the project version matches the version of the dataset you used for training, denoted by `[1]`. In my case, it's `6`.\n",
    "\n",
    "![YOLOv9 Benchmark](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/upload-roboflow-model.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tV-BnNU-7_4h",
    "outputId": "221953fd-3c6d-4cbc-fb5a-b4847fe4dd80"
   },
   "outputs": [],
   "source": [
    "version.deploy(model_type=\"yolov9\", model_path=f\"{HOME}/yolov9/runs/train/exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH30xwvAx1nb"
   },
   "source": [
    "**NOTE:** Now we can download our model anywhere using the assigned `model_id` denoted by `[2]`. In my case `football-players-detection-3zvbc/6`. To download the model you will need your [`ROBOFLOW_API_KEY`](https://docs.roboflow.com/api-reference/authentication).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAB-5ZMM87w3",
    "outputId": "3eb4a88c-0fb5-4dcb-a352-ead516e255a0"
   },
   "outputs": [],
   "source": [
    "ROBOFLOW_API_KEY = getpass.getpass()\n",
    "\n",
    "model = get_model(model_id=\"football-players-detection-3zvbc/8\", api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pGSLZ8Fz5qO"
   },
   "source": [
    "**NOTE:** Let's pick random image from our test subset and detect objects using newly fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aes2oRxi9Kpv"
   },
   "outputs": [],
   "source": [
    "image_paths = sv.list_files_with_extensions(\n",
    "    directory=f\"{dataset.location}/test/images\",\n",
    "    extensions=['png', 'jpg', 'jpeg']\n",
    ")\n",
    "image_path = random.choice(image_paths)\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "result = model.infer(image, confidence=0.1)[0]\n",
    "detections = sv.Detections.from_inference(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Xdr3Vp1uir"
   },
   "source": [
    "**NOTE:** Finally, let's use supervision and [annotate](https://supervision.roboflow.com/develop/annotators/) our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "Kq0BKx1_-kAy",
    "outputId": "cf68a6d0-4d0c-45ae-be58-633d2af6703a"
   },
   "outputs": [],
   "source": [
    "label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "\n",
    "annotated_image = image.copy()\n",
    "annotated_image = bounding_box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_image_filenames(image_folder, output_directory, output_txt_file):\n",
    "    image_filenames = []\n",
    "    for root, dirs, files in os.walk(image_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_filenames.append(file)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Save image filenames to a text file\n",
    "    output_path = os.path.join(output_directory, output_txt_file)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for filename in image_filenames:\n",
    "            f.write(f\"{filename}\\n\")\n",
    "    \n",
    "    # Check if file is created and print message\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Extracted {len(image_filenames)} image filenames and saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to create {output_txt_file}\")\n",
    "\n",
    "# Directory provided by you\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages/frontFar/BLR-2018-03-22_17-39-26_2_frontFar'\n",
    "output_directory = '/home/srmist/Desktop/yolov9'\n",
    "output_txt_file = 'extracted_image_filenames0.txt'  # Output text file name\n",
    "\n",
    "# Run the function\n",
    "extract_image_filenames(image_folder, output_directory, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_xml_filenames(xml_folder):\n",
    "    xml_filenames = []\n",
    "    for root, dirs, files in os.walk(xml_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                xml_filenames.append(file)\n",
    "    return xml_filenames\n",
    "\n",
    "def extract_filenames_from_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    filename = root.find('filename').text\n",
    "    return filename\n",
    "\n",
    "def extract_annotation_filenames(xml_folder):\n",
    "    xml_filenames = extract_xml_filenames(xml_folder)\n",
    "    annotation_filenames = []\n",
    "    for xml_file in xml_filenames:\n",
    "        filename = extract_filenames_from_xml(os.path.join(xml_folder, xml_file))\n",
    "        annotation_filenames.append(filename)\n",
    "    return annotation_filenames\n",
    "\n",
    "def save_filenames_to_txt(filenames, output_folder, output_txt_file):\n",
    "    output_path = os.path.join(output_folder, output_txt_file)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for filename in filenames:\n",
    "            f.write(f\"{filename}\\n\")\n",
    "    print(f\"Extracted {len(filenames)} filenames and saved to {output_path}\")\n",
    "\n",
    "# XML folder location\n",
    "xml_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations/frontFar/BLR-2018-03-22_17-39-26_2_frontFar'\n",
    "\n",
    "# Extract filenames from XML files\n",
    "annotation_filenames = extract_annotation_filenames(xml_folder)\n",
    "\n",
    "# Output folder and text file name\n",
    "output_folder = '/home/srmist/Desktop/yolov9'\n",
    "output_txt_file = 'extracted_annotation_filenames1.txt'\n",
    "\n",
    "# Save extracted filenames to text file\n",
    "save_filenames_to_txt(annotation_filenames, output_folder, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_folder(folder):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.relpath(os.path.join(root, filename), folder))\n",
    "    return set(files)\n",
    "\n",
    "def compare_folders(folder1, folder2):\n",
    "    files1 = list_files_in_folder(folder1)\n",
    "    files2 = list_files_in_folder(folder2)\n",
    "    missing_files_in_folder2 = files1 - files2\n",
    "    return missing_files_in_folder2\n",
    "\n",
    "# Folders to compare\n",
    "folder1 = '/home/srmist/Desktop/yolov9/extracted_annotation_filenames1.txt'\n",
    "folder2 = '/home/srmist/Desktop/yolov9/extracted_annotation_filenames0.txt'\n",
    "\n",
    "# Compare folders and get missing files\n",
    "missing_files = compare_folders(folder1, folder2)\n",
    "\n",
    "# Print missing files\n",
    "if missing_files:\n",
    "    print(\"Missing files in folder2:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No missing files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_folder(folder):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.relpath(os.path.join(root, filename), folder))\n",
    "    return set(files)\n",
    "\n",
    "def extract_common_subfolders(image_folder, annotation_folder):\n",
    "    image_files = list_files_in_folder(image_folder)\n",
    "    annotation_files = list_files_in_folder(annotation_folder)\n",
    "    common_subfolders = set()\n",
    "    for image_file in image_files:\n",
    "        subfolder = os.path.dirname(image_file)\n",
    "        if subfolder in annotation_files:\n",
    "            common_subfolders.add(subfolder)\n",
    "    return common_subfolders\n",
    "\n",
    "def check_corresponding_files(image_folder, annotation_folder, common_subfolders):\n",
    "    missing_annotations = []\n",
    "    for subfolder in common_subfolders:\n",
    "        image_files = list_files_in_folder(os.path.join(image_folder, subfolder))\n",
    "        annotation_files = list_files_in_folder(os.path.join(annotation_folder, subfolder))\n",
    "        for image_file in image_files:\n",
    "            annotation_file = os.path.splitext(image_file)[0] + '.xml'\n",
    "            if annotation_file not in annotation_files:\n",
    "                missing_annotations.append(os.path.join(subfolder, annotation_file))\n",
    "    return missing_annotations\n",
    "\n",
    "# Folders to compare\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations/Annotations'\n",
    "\n",
    "# Extract common subfolders\n",
    "common_subfolders = extract_common_subfolders(image_folder, annotation_folder)\n",
    "\n",
    "# Check corresponding files\n",
    "missing_annotations = check_corresponding_files(image_folder, annotation_folder, common_subfolders)\n",
    "\n",
    "# Print missing annotation files\n",
    "if missing_annotations:\n",
    "    print(\"Missing annotation files for some images:\")\n",
    "    for annotation_file in missing_annotations:\n",
    "        print(annotation_file)\n",
    "else:\n",
    "    print(\"All images have corresponding annotation files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_files_from_txt(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder):\n",
    "    # Read filenames from the txt file\n",
    "    with open(txt_file, 'r') as f:\n",
    "        filenames = f.read().splitlines()\n",
    "\n",
    "    # Copy images and annotations to output folders\n",
    "    for filename in filenames:\n",
    "        image_path = find_file_recursive(image_folder, filename)\n",
    "        annotation_path = find_file_recursive(annotation_folder, os.path.splitext(filename)[0] + '.xml')\n",
    "        if image_path and annotation_path:\n",
    "            shutil.copy(image_path, output_image_folder)\n",
    "            shutil.copy(annotation_path, output_label_folder)\n",
    "            print(f\"Processed: {filename}\")\n",
    "        else:\n",
    "            print(f\"File not found: {filename}\")\n",
    "    \n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "def find_file_recursive(folder, filename):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file == filename:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "# Input parameters\n",
    "txt_file = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/labels'\n",
    "\n",
    "# Extract files from txt\n",
    "extract_files_from_txt(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_image_locations(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder):\n",
    "    # Read image locations from the txt file\n",
    "    with open(txt_file, 'r') as f:\n",
    "        image_locations = f.read().splitlines()\n",
    "\n",
    "    # Process each image location\n",
    "    for location in image_locations:\n",
    "        # Extract image filename\n",
    "        image_filename = os.path.basename(location)\n",
    "\n",
    "        # Find image and annotation paths\n",
    "        image_path = find_file_recursive(image_folder, image_filename)\n",
    "        annotation_path = find_file_recursive(annotation_folder, os.path.splitext(image_filename)[0] + '.xml')\n",
    "\n",
    "        # If both image and annotation exist, copy them to output folders\n",
    "        if image_path and annotation_path:\n",
    "            shutil.copy(image_path, output_image_folder)\n",
    "            shutil.copy(annotation_path, output_label_folder)\n",
    "            print(f\"Processed: {image_filename}\")\n",
    "        else:\n",
    "            print(f\"Image or annotation not found for: {image_filename}\")\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "def find_file_recursive(folder, filename):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file == filename:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "# Input parameters\n",
    "txt_file = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/labels'\n",
    "\n",
    "# Process image locations\n",
    "process_image_locations(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_image_locations(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder):\n",
    "    # Read image locations from the txt file\n",
    "    with open(txt_file, 'r') as f:\n",
    "        image_locations = f.read().splitlines()\n",
    "\n",
    "    # Process each image location\n",
    "    for location in image_locations:\n",
    "        # Split the location into folders and filename\n",
    "        folders, image_filename = os.path.split(location)\n",
    "\n",
    "        # Construct the full path to the image and annotation\n",
    "        image_path = os.path.join(image_folder, folders, image_filename)\n",
    "        annotation_path = os.path.join(annotation_folder, folders, os.path.splitext(image_filename)[0] + '.xml')\n",
    "\n",
    "        # If the image exists and annotation exists, copy them to output folders\n",
    "        if os.path.exists(image_path) and os.path.exists(annotation_path):\n",
    "            shutil.copy(image_path, output_image_folder)\n",
    "            shutil.copy(annotation_path, output_label_folder)\n",
    "            print(f\"Processed: {location}\")\n",
    "        else:\n",
    "            print(f\"Image or annotation not found for: {location}\")\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "# Input parameters\n",
    "txt_file = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/labels'\n",
    "\n",
    "# Process image locations\n",
    "process_image_locations(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_folders_from_txt(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder):\n",
    "    # Read folder paths from the txt file\n",
    "    with open(txt_file, 'r') as f:\n",
    "        folder_paths = f.read().splitlines()\n",
    "\n",
    "    # Process each folder path\n",
    "    for folder_path in folder_paths:\n",
    "        process_folder(folder_path, image_folder, annotation_folder, output_image_folder, output_label_folder)\n",
    "\n",
    "def process_folder(folder_path, image_folder, annotation_folder, output_image_folder, output_label_folder):\n",
    "    # Traverse through the image and annotation folders based on the provided folder path\n",
    "    for root, dirs, files in os.walk(os.path.join(image_folder, folder_path)):\n",
    "        for file in files:\n",
    "            # Construct the full path to the image file\n",
    "            image_file_path = os.path.join(root, file)\n",
    "\n",
    "            # Construct the corresponding annotation file path\n",
    "            annotation_file_path = os.path.join(annotation_folder, folder_path, os.path.splitext(file)[0] + '.xml')\n",
    "\n",
    "            # Check if the annotation file exists\n",
    "            if os.path.exists(annotation_file_path):\n",
    "                # Copy the image and annotation files to the output folders\n",
    "                shutil.copy(image_file_path, output_image_folder)\n",
    "                shutil.copy(annotation_file_path, output_label_folder)\n",
    "                print(f\"Processed: {image_file_path}\")\n",
    "            else:\n",
    "                print(f\"Annotation file not found for: {image_file_path}\")\n",
    "\n",
    "# Input parameters\n",
    "txt_file = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/labels'\n",
    "\n",
    "# Process folders from the text file\n",
    "process_folders_from_txt(txt_file, image_folder, annotation_folder, output_image_folder, output_label_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "txt_file = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/images'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train/labels'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_folder, exist_ok=True)\n",
    "os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "# Read the text file containing image filenames\n",
    "with open(txt_file, 'r') as file:\n",
    "    image_filenames = file.readlines()\n",
    "\n",
    "# Process each image filename\n",
    "for image_filename in image_filenames:\n",
    "    image_filename = image_filename.strip()\n",
    "    # Get the full path of the image\n",
    "    image_path = os.path.join(image_folder, image_filename + '.jpg')\n",
    "    annotation_path = os.path.join(annotation_folder, image_filename + '.txt')\n",
    "\n",
    "    # Check if both image and annotation files exist\n",
    "    if os.path.exists(image_path) and os.path.exists(annotation_path):\n",
    "        # Copy image to the output image folder\n",
    "        shutil.copy(image_path, os.path.join(output_image_folder, image_filename + '.jpg'))\n",
    "        print(f\"Copied image: {image_filename}.jpg\")\n",
    "        # Copy annotation to the output label folder\n",
    "        shutil.copy(annotation_path, os.path.join(output_label_folder, image_filename + '.txt'))\n",
    "        print(f\"Copied annotation: {image_filename}.txt\")\n",
    "    else:\n",
    "        print(f\"Image or annotation not found for {image_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def gather_files_from_subfolders(images_folder, annotations_folder, image_extension):\n",
    "    \"\"\"Gather all image files with the given extension from the subfolders if they have a corresponding annotation.\"\"\"\n",
    "    files = []\n",
    "    for subfolder in os.listdir(images_folder):\n",
    "        subfolder_path = os.path.join(images_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for root, _, filenames in os.walk(subfolder_path):\n",
    "                for filename in filenames:\n",
    "                    if filename.endswith(image_extension):\n",
    "                        image_file = os.path.join(root, filename)\n",
    "                        annotation_file = os.path.join(annotations_folder, os.path.relpath(image_file, images_folder).replace(image_extension, '.xml'))\n",
    "                        if os.path.exists(annotation_file):\n",
    "                            files.append((image_file, annotation_file))\n",
    "    return files\n",
    "\n",
    "def split_files(files, train_ratio=0.7, val_ratio=0.2):\n",
    "    \"\"\"Split files into train, validation, and test sets.\"\"\"\n",
    "    random.shuffle(files)\n",
    "    num_files = len(files)\n",
    "    train_end = int(train_ratio * num_files)\n",
    "    val_end = int((train_ratio + val_ratio) * num_files)\n",
    "    return files[:train_end], files[train_end:val_end], files[val_end:]\n",
    "\n",
    "def copy_files(file_pairs, dst_images_folder, dst_labels_folder):\n",
    "    \"\"\"Copy files and their corresponding labels to the destination folder.\"\"\"\n",
    "    for image_file, annotation_file in file_pairs:\n",
    "        filename = os.path.basename(image_file)\n",
    "        try:\n",
    "            # Copy image\n",
    "            shutil.copy(image_file, os.path.join(dst_images_folder, filename))\n",
    "            print(f\"Copied image: {image_file} to {dst_images_folder}\")\n",
    "\n",
    "            # Copy corresponding annotation\n",
    "            shutil.copy(annotation_file, os.path.join(dst_labels_folder, filename.replace('.jpg', '.xml')))\n",
    "            print(f\"Copied annotation: {annotation_file} to {dst_labels_folder}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying files {image_file} and {annotation_file}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    src_images_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "    src_annotations_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "    output_folder = '/home/srmist/Desktop/yolov9/output'\n",
    "\n",
    "    # Create train, validation, and test directories\n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        os.makedirs(os.path.join(output_folder, subset, 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_folder, subset, 'labels'), exist_ok=True)\n",
    "\n",
    "    # Gather all image files from subfolders\n",
    "    print(\"Gathering image files...\")\n",
    "    image_files = gather_files_from_subfolders(src_images_folder, src_annotations_folder, '.jpg')\n",
    "    print(f\"Total images found: {len(image_files)}\")\n",
    "\n",
    "    # Split files into train, validation, and test sets\n",
    "    print(\"Splitting files into train, validation, and test sets...\")\n",
    "    train_files, val_files, test_files = split_files(image_files)\n",
    "    print(f\"Train set size: {len(train_files)}\")\n",
    "    print(f\"Validation set size: {len(val_files)}\")\n",
    "    print(f\"Test set size: {len(test_files)}\")\n",
    "\n",
    "    # Copy files to their respective directories\n",
    "    print(\"Copying train files...\")\n",
    "    copy_files(train_files, os.path.join(output_folder, 'train', 'images'), os.path.join(output_folder, 'train', 'labels'))\n",
    "    \n",
    "    print(\"Copying validation files...\")\n",
    "    copy_files(val_files, os.path.join(output_folder, 'validation', 'images'), os.path.join(output_folder, 'validation', 'labels'))\n",
    "    \n",
    "    print(\"Copying test files...\")\n",
    "    copy_files(test_files, os.path.join(output_folder, 'test', 'images'), os.path.join(output_folder, 'test', 'labels'))\n",
    "\n",
    "    print(\"Process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_xml_to_txt(xml_file, output_folder):\n",
    "    \"\"\"Convert a single XML file to TXT format.\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    txt_content = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = bndbox.find('xmin').text\n",
    "        ymin = bndbox.find('ymin').text\n",
    "        xmax = bndbox.find('xmax').text\n",
    "        ymax = bndbox.find('ymax').text\n",
    "\n",
    "        txt_content.append(f\"{name} {xmin} {ymin} {xmax} {ymax}\")\n",
    "\n",
    "    txt_filename = os.path.basename(xml_file).replace('.xml', '.txt')\n",
    "    output_path = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write('\\n'.join(txt_content))\n",
    "\n",
    "    print(f\"Converted {xml_file} to {output_path}\")\n",
    "\n",
    "def convert_all_xmls_in_folder(input_folder, output_folder):\n",
    "    \"\"\"Convert all XML files in the given folder and its subfolders to TXT format.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output directory: {output_folder}\")\n",
    "    \n",
    "    for root, _, filenames in os.walk(input_folder):\n",
    "        print(f\"Entering directory: {root}\")\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.xml'):\n",
    "                xml_file = os.path.join(root, filename)\n",
    "                print(f\"Processing file: {xml_file}\")\n",
    "                convert_xml_to_txt(xml_file, output_folder)\n",
    "\n",
    "def main():\n",
    "    input_folder = '/home/srmist/Desktop/yolov9/IDD dataset/train/labels'\n",
    "    output_folder = '/home/srmist/Desktop/yolov9/IDD dataset/train/labels1'\n",
    "    \n",
    "    print(\"Starting conversion of XML files to TXT format...\")\n",
    "    convert_all_xmls_in_folder(input_folder, output_folder)\n",
    "    print(\"Conversion process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_items_in_folder(folder):\n",
    "    \"\"\"Count the number of items (files and directories) in the given folder.\"\"\"\n",
    "    items = os.listdir(folder)\n",
    "    num_items = len(items)\n",
    "    return num_items\n",
    "\n",
    "def main():\n",
    "    folder_path = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/train.txt'\n",
    "    num_items = count_items_in_folder(folder_path)\n",
    "    print(f\"Number of items in folder '{folder_path}': {num_items}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to parse XML annotation files\n",
    "def parse_xml_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract bounding box coordinates and labels\n",
    "    annotations = []\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        label = obj.find('name').text\n",
    "        annotations.append((xmin, ymin, xmax, ymax, label))\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "# Function to traverse through directories and collect image paths and annotations\n",
    "def collect_data(image_dir, annotation_dir):\n",
    "    data = []\n",
    "\n",
    "    for subdir, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                annotation_file = os.path.join(annotation_dir, subdir.split('/')[-1], file.replace('.jpg', '.xml'))\n",
    "                \n",
    "                if os.path.exists(annotation_file):\n",
    "                    # Parse XML annotation\n",
    "                    annotations = parse_xml_annotation(annotation_file)\n",
    "                    \n",
    "                    # Load image\n",
    "                    image = cv2.imread(image_path)\n",
    "                    \n",
    "                    # Combine image and annotations\n",
    "                    data.append((image, annotations))\n",
    "\n",
    "    return data\n",
    "\n",
    "# Folder locations\n",
    "image_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/JPEGImages'\n",
    "annotation_folder = '/home/srmist/Desktop/yolov9/idd-detection/IDD_Detection/Annotations'\n",
    "output_image_folder = '/home/srmist/Desktop/yolov9'\n",
    "output_label_folder = '/home/srmist/Desktop/yolov9'\n",
    "\n",
    "print(\"Collecting data...\")\n",
    "# Load images and annotations, and combine them\n",
    "data = collect_data(image_folder, annotation_folder)\n",
    "print(\"Data collection completed.\")\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=1/3, random_state=42)\n",
    "print(\"Dataset split completed.\")\n",
    "\n",
    "# Function to save images and annotations\n",
    "def save_data(data, image_dir, annotation_dir):\n",
    "    image_save_dir = os.path.join(output_image_folder, image_dir)\n",
    "    annotation_save_dir = os.path.join(output_label_folder, annotation_dir)\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "    os.makedirs(annotation_save_dir, exist_ok=True)\n",
    "\n",
    "    for idx, (image, annotations) in enumerate(data):\n",
    "        # Save image\n",
    "        image_filename = os.path.join(image_save_dir, f\"image_{idx}.jpg\")\n",
    "        cv2.imwrite(image_filename, image)\n",
    "        \n",
    "        # Save annotations in XML format\n",
    "        annotation_filename = os.path.join(annotation_save_dir, f\"label_{idx}.xml\")\n",
    "        root = ET.Element(\"annotations\")\n",
    "        for xmin, ymin, xmax, ymax, label in annotations:\n",
    "            obj = ET.SubElement(root, \"object\")\n",
    "            ET.SubElement(obj, \"name\").text = label\n",
    "            bbox = ET.SubElement(obj, \"bndbox\")\n",
    "            ET.SubElement(bbox, \"xmin\").text = str(xmin)\n",
    "            ET.SubElement(bbox, \"ymin\").text = str(ymin)\n",
    "            ET.SubElement(bbox, \"xmax\").text = str(xmax)\n",
    "            ET.SubElement(bbox, \"ymax\").text = str(ymax)\n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(annotation_filename)\n",
    "\n",
    "print(\"Saving data...\")\n",
    "# Save train, validation, and test data\n",
    "save_data(train_data, \"train_images\", \"train_labels\")\n",
    "save_data(val_data, \"validation_images\", \"validation_labels\")\n",
    "save_data(test_data, \"test_images\", \"test_labels\")\n",
    "\n",
    "print(\"Data preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "def print_tensorrt_info(engine_file_path):\n",
    "    # Load the TensorRT engine\n",
    "    with open(engine_file_path, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.INFO)) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "    # Print general information\n",
    "    print(\"TensorRT Engine Information:\")\n",
    "    print(\"============================\")\n",
    "    print(f\"Name: {engine.network_name}\")\n",
    "    print(f\"Platform: {engine.platform}\")\n",
    "    print(f\"TensorRT Version: {engine.get_version()}\")\n",
    "\n",
    "    # Print binding information\n",
    "    bindings = [engine.get_binding_name(i) for i in range(engine.num_bindings)]\n",
    "    print(\"\\nBindings:\")\n",
    "    print(\"---------\")\n",
    "    for i, binding in enumerate(bindings):\n",
    "        print(f\"Binding {i}: {binding}\")\n",
    "\n",
    "    # Print optimization settings\n",
    "    print(\"\\nOptimization Settings:\")\n",
    "    print(\"----------------------\")\n",
    "    print(f\"Max Batch Size: {engine.max_batch_size}\")\n",
    "    print(f\"Max Workspace Size: {engine.max_workspace_size}\")\n",
    "\n",
    "    # Print layer information\n",
    "    print(\"\\nLayers:\")\n",
    "    print(\"-------\")\n",
    "    for i in range(engine.num_layers):\n",
    "        layer = engine.get_layer(i)\n",
    "        print(f\"Layer {i}:\")\n",
    "        print(f\"  Name: {layer.name}\")\n",
    "        print(f\"  Type: {layer.type}\")\n",
    "        print(f\"  Precision: {layer.precision}\")\n",
    "        print(f\"  Input Shape: {layer.get_input(0).shape}\")\n",
    "        print(f\"  Output Shape: {layer.get_output(0).shape}\")\n",
    "        # Add more layer properties as needed\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engine_file_path = \"/home/srmist/Desktop/yolov9/yolov9.engine\"\n",
    "    print_tensorrt_info(engine_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> 36bd69c (Initial commit)
